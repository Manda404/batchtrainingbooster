# BatchTrainingBooster

**Framework d'entra√Ænement incr√©mental par batch avec XGBoost/CatBoost et Apache Spark**

## üéØ Objectif du Projet

BatchTrainingBooster est un framework con√ßu pour r√©soudre les d√©fis de l'entra√Ænement de mod√®les de machine learning sur de tr√®s grandes donn√©es distribu√©es. Il combine la puissance d'Apache Spark pour la gestion des donn√©es avec l'efficacit√© des algorithmes de gradient boosting (XGBoost, CatBoost) pour cr√©er un syst√®me d'entra√Ænement incr√©mental par batch.

### üîç Probl√©matique Adress√©e

- **Donn√©es massives** : Impossibilit√© de charger l'ensemble des donn√©es en m√©moire
- **Entra√Ænement s√©quentiel** : N√©cessit√© d'entra√Æner sur des sous-ensembles de donn√©es de mani√®re coh√©rente
- **Optimisation continue** : Ajustement dynamique des hyperparam√®tres (learning rate, early stopping)
- **√âquilibrage des classes** : Maintien de la distribution des classes dans chaque batch

## üèóÔ∏è Architecture

Le framework s'articule autour de plusieurs composants cl√©s :
```
batchtrainingbooster/
‚îÇ
‚îú‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py               # expose OptimizedWeightCalculator
‚îÇ   ‚îú‚îÄ‚îÄ base_trainer.py           # classe abstraite: fit(), get_trained_model(), save(), load(), etc.
‚îÇ   ‚îî‚îÄ‚îÄ weights.py                # OptimizedWeightCalculator (cache, smoothing, normalize, labels_all)
‚îÇ
‚îú‚îÄ‚îÄ trainers/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ xgboost_trainer.py        # utilise OptimizedWeightCalculator -> sample_weight par batch
‚îÇ   ‚îî‚îÄ‚îÄ catboost_trainer.py       # idem pour CatBoost (fit avec sample_weight)
‚îÇ
‚îî‚îÄ‚îÄ logger/
    ‚îî‚îÄ‚îÄ logger.py                 # logger centralis√© (handlers, niveaux, format)
```

## ‚ö° Fonctionnalit√©s Impl√©ment√©es

### 1. **Entra√Ænement Incr√©mental par Batch**
- **Stratification automatique** : Chaque batch maintient la distribution des classes cibles
- **Ordonnancement al√©atoire** : Randomisation avec seed pour la reproductibilit√©
- **Warm restart** : Continuit√© de l'entra√Ænement entre les batches

### 2. **Optimisation Avanc√©e**
- **Learning Rate Scheduling** : D√©croissance exponentielle automatique
- **Early Stopping Global** : Arr√™t bas√© sur la performance sur l'ensemble de validation
- **√âquilibrage des classes** : Calcul automatique des poids d'√©chantillonnage

### 3. **Monitoring et Visualisation**
- **Courbes d'apprentissage** : Visualisation des m√©triques train/validation
- **Suivi des hyperparam√®tres** : √âvolution du learning rate par batch
- **Logging d√©taill√©** : Tra√ßabilit√© compl√®te du processus d'entra√Ænement

### 4. **Support Multi-Mod√®les**

#### XGBoostTrainer
```python
trainer = XGBoostTrainer()
model = trainer.fit(
    train_dataframe=spark_train_df,
    valid_dataframe=spark_valid_df,
    target_column="NObeyesdad",
    config_model={
        "objective": "multi:softprob",
        "n_estimators": 100,
        "num_class": spark_valid_df.select("NObeyesdad").distinct().count(),
        "max_depth": 6,
        "eval_metric": "mlogloss"
    },
    config_training = {
        "num_batches": 2,  # nombre de lots pour l'entra√Ænement
        "max_patience": 5,  # patience pour early stopping global
        "show_learning_curve": True,
    },
    config_lr_scheduler={
        "initial_lr": 0.1,
        "decay_rate": 0.95
    }
)