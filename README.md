# BatchTrainingBooster

**Framework d'entraÃ®nement incrÃ©mental par batch avec XGBoost/CatBoost et Apache Spark**

## ğŸ¯ Objectif du Projet

BatchTrainingBooster est un framework conÃ§u pour rÃ©soudre les dÃ©fis de l'entraÃ®nement de modÃ¨les de machine learning sur de trÃ¨s grandes donnÃ©es distribuÃ©es. Il combine la puissance d'Apache Spark pour la gestion des donnÃ©es avec l'efficacitÃ© des algorithmes de gradient boosting (XGBoost, CatBoost) pour crÃ©er un systÃ¨me d'entraÃ®nement incrÃ©mental par batch.

### ğŸ” ProblÃ©matique AdressÃ©e

- **DonnÃ©es massives** : ImpossibilitÃ© de charger l'ensemble des donnÃ©es en mÃ©moire
- **EntraÃ®nement sÃ©quentiel** : NÃ©cessitÃ© d'entraÃ®ner sur des sous-ensembles de donnÃ©es de maniÃ¨re cohÃ©rente
- **Optimisation continue** : Ajustement dynamique des hyperparamÃ¨tres (learning rate, early stopping)
- **Ã‰quilibrage des classes** : Maintien de la distribution des classes dans chaque batch

## ğŸ—ï¸ Architecture

Le framework s'articule autour de plusieurs composants clÃ©s :
src/batchtrainingbooster/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ base_trainer.py          # Classe abstraite pour tous les trainers
â”œâ”€â”€ trainers/
â”‚   â”œâ”€â”€ xgboost_trainer.py       # ImplÃ©mentation XGBoost avec warm restart
â”‚   â””â”€â”€ catboost_trainer.py      # ImplÃ©mentation CatBoost avec modÃ¨le incrÃ©mental
â”œâ”€â”€ xyz/
â”‚   â””â”€â”€ .py       #
â””â”€â”€ logger/
    â””â”€â”€ logger.py                # SystÃ¨me de logging centralisÃ©


## âš¡ FonctionnalitÃ©s ImplÃ©mentÃ©es

### 1. **EntraÃ®nement IncrÃ©mental par Batch**
- **Stratification automatique** : Chaque batch maintient la distribution des classes cibles
- **Ordonnancement alÃ©atoire** : Randomisation avec seed pour la reproductibilitÃ©
- **Warm restart** : ContinuitÃ© de l'entraÃ®nement entre les batches

### 2. **Optimisation AvancÃ©e**
- **Learning Rate Scheduling** : DÃ©croissance exponentielle automatique
- **Early Stopping Global** : ArrÃªt basÃ© sur la performance sur l'ensemble de validation
- **Ã‰quilibrage des classes** : Calcul automatique des poids d'Ã©chantillonnage

### 3. **Monitoring et Visualisation**
- **Courbes d'apprentissage** : Visualisation des mÃ©triques train/validation
- **Suivi des hyperparamÃ¨tres** : Ã‰volution du learning rate par batch
- **Logging dÃ©taillÃ©** : TraÃ§abilitÃ© complÃ¨te du processus d'entraÃ®nement

### 4. **Support Multi-ModÃ¨les**

#### XGBoostTrainer
```python
trainer = XGBoostTrainer()
model = trainer.fit(
    train_dataframe=spark_train_df,
    valid_dataframe=spark_valid_df,
    target_column="NObeyesdad",
    config_model={
        "n_estimators": 100,
        "max_depth": 6,
        "eval_metric": "mlogloss"
    },
    config_training = {
        "num_batches": 2,  # nombre de lots pour l'entraÃ®nement
        "max_patience": 5,  # patience pour early stopping global
        "show_learning_curve": True,
    },
    config_lr_scheduler={
        "initial_lr": 0.1,
        "decay_rate": 0.95
    }
)